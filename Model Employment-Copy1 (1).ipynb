{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import datetime as dt\n",
    "import csv\n",
    "\n",
    "from scipy import sparse\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso, LinearRegression, LogisticRegression, RidgeClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Loading dataset + defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_df = pd.read_csv('datasets\\\\science_dataset_updated2.csv') #Loads dataset\n",
    "sports_df = pd.read_csv('datasets\\\\sports_dataset_updated2.csv') #Loads dataset\n",
    "gaming_df = pd.read_csv('datasets\\\\gaming_dataset_updated2.csv') #Loads dataset\n",
    "wsb_df = pd.read_csv('datasets\\\\wsb_dataset_updated2.csv') #Loads dataset\n",
    "music_df = pd.read_csv('datasets\\\\music_dataset_updated.csv') #Loads dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original title:  Minecraft Map Banned In 20+ Countries\n",
      "Processed title:  ['minecraft', 'map', 'banned', 'countries']\n"
     ]
    }
   ],
   "source": [
    "print('Original title: ', gaming_df.title[1])\n",
    "print('Processed title: ', gaming_df.title_cleaned[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) #Defines stopwords\n",
    "ps = PorterStemmer() #Defines stemmer\n",
    "\n",
    "def preprocess_text_col(dataframe, column_name): #Function for preprocessing text data for model-use by adding 'title-cleaned' column to given dataframe\n",
    "    def remove_punctuation(text): #Removes punctuation from string e.g. 'This is a string. This is another string' → 'this is a string This is another string' \n",
    "        no_punct=[words.lower() for words in text if words not in string.punctuation and words.isdigit() == False]\n",
    "        words_wo_punct=''.join(no_punct)\n",
    "        return words_wo_punct\n",
    "    def tokenize(text): #Tokenizes string e.g. 'This is a string' → ['this', 'is', 'a', 'string']\n",
    "        split=re.split(\"\\W+\", text) \n",
    "        return split\n",
    "    def remove_stopwords(text): #Removes stopwords list of strings e.g. ['this', 'is', 'a', 'string'] → ['string']\n",
    "        text=[word for word in text if word not in stop_words]\n",
    "        return text\n",
    "    def stem_nested_list(lst): #Stems words in a nested list and returns a nested list with stemmed words\n",
    "        master_list = []\n",
    "        for x in lst:\n",
    "            stemmed_list = [ps.stem(word) for word in x]\n",
    "            master_list.append(stemmed_list)\n",
    "        return master_list\n",
    "    \n",
    "    title_wo_punct = [remove_punctuation(x) for x in dataframe[column_name]]\n",
    "    title_wo_punct_split = [tokenize(word) for word in title_wo_punct]\n",
    "    title_wo_punct_split_stopwords = [remove_stopwords(word) for word in title_wo_punct_split]\n",
    "    dataframe['title_cleaned'] = title_wo_punct_split_stopwords\n",
    "#     dataframe['title_cleaned'] = stem_nested_list(title_wo_punct_split_stopwords)    \n",
    "\n",
    "def create_features(dataframe):\n",
    "    dataframe['timestamp'] = pd.to_datetime(dataframe['timestamp']) #Changing 'timestamp' column to dtype = datetime\n",
    "    dataframe['24h_posttime'] = dataframe['timestamp'].dt.hour #Adding hour posttime to dataset\n",
    "    \n",
    "    dataframe['score_class'] = \"\" #Creating the score_class column in the dataframe and filling it with empty strings\n",
    "    \n",
    "    dataframe['body'] = dataframe['body'].astype(str)\n",
    "    dataframe.loc[(dataframe['body'] == 'nan') | (dataframe['body'] == '[deleted]'), 'has_body_text'] = int(0) \n",
    "    dataframe.loc[(dataframe['body'] != 'nan') & (dataframe['body'] != '[deleted]'), 'has_body_text'] = int(1)\n",
    "#     dataframe['has_body_text'] = dataframe['has_body_text'].astype(int)\n",
    "    \n",
    "    for x in range(len(dataframe)): #Generates classes for score percentiles\n",
    "        if dataframe['score'][x] >= dataframe.score.quantile(0.99):\n",
    "            dataframe['score_class'][x] = 'Top 1%'\n",
    "        elif dataframe['score'][x] >= dataframe.score.quantile(0.95):\n",
    "            dataframe['score_class'][x] = 'Top 5%'\n",
    "        elif dataframe['score'][x] >= dataframe.score.quantile(0.9):\n",
    "            dataframe['score_class'][x] = 'Top 10%'\n",
    "        elif dataframe['score'][x] >= dataframe.score.quantile(0.8):\n",
    "            dataframe['score_class'][x] = 'Top 20%'\n",
    "        else:\n",
    "            dataframe['score_class'][x] = 'Last 80%'\n",
    "    \n",
    "def test_model(model): #Function for testing model(s)\n",
    "    if type(model) == list:\n",
    "        for x in range(len(model)):\n",
    "            print(\"Training score for {}: {:.3f}\".format(str(model[x]), model[x].score(X_train, y_train)))\n",
    "            print(\"Test score for {}: {:.2f}\\n\".format(str(model[x]), model[x].score(X_test, y_test)))\n",
    "    else:\n",
    "        print(\"Training score for {}: {:.3f}\".format(str(model), model.score(X_train, y_train)))\n",
    "        print(\"Test score for {}: {:.2f}\".format(str(model), model.score(X_test, y_test)))\n",
    "        \n",
    "def col_to_matrix(dataframe, column): #Function for converting a column from a pd.dataframe into a scipy.sparse.csr_matrix\n",
    "    matrix = dataframe[column].values[np.newaxis] #Creating 2D np array from column by adding an axis to original 1D array (df[col].values)\n",
    "    matrix = matrix.T #Transposing (rotating) array e.g. (1, 823) to (823, 1)\n",
    "    matrix = sparse.csr_matrix(matrix) #Creating matrix from array\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Preprocessing text-data for model-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent (hh:mm:ss): 0:00:00.637542\n"
     ]
    }
   ],
   "source": [
    "begin_time = dt.datetime.now()\n",
    "\n",
    "preprocess_text_col(science_df, 'title')\n",
    "score_list = sorted([x for x in science_df.score]) #Generates a list of all scores sorted in ascending order\n",
    "\n",
    "post_time = col_to_matrix(science_df, '24h_posttime')\n",
    "title_len = col_to_matrix(science_df, 'title length')\n",
    "has_body_text = col_to_matrix(science_df, 'has_body_text')\n",
    "\n",
    "vectorizeruni = CountVectorizer(lowercase=False, analyzer=lambda x: x, ngram_range = (1,1))\n",
    "vectorizerbi = CountVectorizer(lowercase=False, analyzer=lambda x: x, ngram_range = (2,2))\n",
    "vectorizertri = CountVectorizer(lowercase=False, analyzer=lambda x: x, ngram_range = (3,3))\n",
    "\n",
    "vectorizer = vectorizeruni\n",
    "\n",
    "tfidfvectorizer = TfidfVectorizer(lowercase = False, analyzer=lambda x: x)\n",
    "\n",
    "dataframe = science_df\n",
    "\n",
    "titles_vectorized_tf = tfidfvectorizer.fit_transform(dataframe.title_cleaned)\n",
    "titles_vectorized_bow = vectorizer.fit_transform(dataframe.title_cleaned)\n",
    "\n",
    "titles_vectorized_tf = sparse.hstack((post_time, titles_vectorized_tf)) #Adding posttime column to matrix\n",
    "titles_vectorized_tf = sparse.hstack((title_len, titles_vectorized_tf)) #Adding title_length column to matrix\n",
    "titles_vectorized_tf = sparse.hstack((has_body_text, titles_vectorized_tf)) #Adding has_body_text column to matrix\n",
    "\n",
    "titles_vectorized_bow = sparse.hstack((post_time, titles_vectorized_bow)) #Adding posttime column to matrix\n",
    "titles_vectorized_bow = sparse.hstack((title_len, titles_vectorized_bow)) #Adding title_length column to matrix\n",
    "titles_vectorized_bow = sparse.hstack((has_body_text, titles_vectorized_bow)) #Adding has_body_text column to matrix\n",
    "\n",
    "print('Time spent (hh:mm:ss):', dt.datetime.now() - begin_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Creating, training and testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge done 2021-05-25 14:58:21.254846\n",
      "svc done 2021-05-25 14:58:32.019120\n",
      "knn done 2021-05-25 14:58:41.231536\n"
     ]
    }
   ],
   "source": [
    "begin_time = dt.datetime.now()\n",
    "\n",
    "target = science_df.score_class\n",
    "X_train, X_test, y_train, y_test = train_test_split(titles_vectorized_bow, target, test_size = 0.2, random_state = 2021)\n",
    "\n",
    "results = {}\n",
    "for x in [0.01, 0.1, 1, 5, 10, 20]:\n",
    "    ridge = RidgeClassifier(alpha = x, max_iter=5000).fit(X_train, y_train)\n",
    "    results['ridge_training_score '+ str(x)] = ridge.score(X_train, y_train)\n",
    "    results['ridge_test_score '+ str(x)] = ridge.score(X_test, y_test)\n",
    "print('ridge done '+dt.datetime.now() - begin_time)\n",
    "\n",
    "SVC_model = SVC().fit(X_train, y_train)\n",
    "results['svc_training_score'] = SVC_model.score(X_train, y_train)\n",
    "results['svc_test_score'] = SVC_model.score(X_test, y_test)\n",
    "print('svc done '+dt.datetime.now() - begin_time)\n",
    "\n",
    "for x in [1, 2, 3, 4, 5]:\n",
    "    knn = KNeighborsClassifier(n_neighbors = x).fit(X_train, y_train)\n",
    "    results['knn_training_score '+ str(x)] = knn.score(X_train, y_train)\n",
    "    results['knn_test_score '+ str(x)] = knn.score(X_test, y_test)\n",
    "print('knn done '+dt.datetime.now() - begin_time)\n",
    "\n",
    "for x in [20, 50, 70, 100]:\n",
    "    for y in [50, 100, 200, 300]:\n",
    "        forest = RandomForestClassifier(n_estimators = x, max_depth = y).fit(X_train, y_train)\n",
    "        dtree = DecisionTreeClassifier(max_depth = y).fit(X_train, y_train)\n",
    "        results['forest_training_score trees:'+ str(x) + ' dpth: '+str(y)] = forest.score(X_train, y_train)\n",
    "        results['forest_test_score trees:'+ str(x) + ' dpth: '+str(y)] = forest.score(X_test, y_test)\n",
    "        results['dtree_training_score ' + ' dpth: '+str(y)] = dtree.score(X_train, y_train)\n",
    "        results['dtree_test_score ' + ' dpth: '+str(y)] = dtree.score(X_test, y_test)\n",
    "        \n",
    "train_end_time = dt.datetime.now()\n",
    "print('Time spent training (hh:mm:ss):', train_end_time - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge_training_score 0.01 0.9931565440547476\n",
      "ridge_test_score 0.01 0.669327251995439\n",
      "ridge_training_score 0.1 0.9924436840604506\n",
      "ridge_test_score 0.1 0.7029646522234891\n",
      "ridge_training_score 1 0.9888793840889649\n",
      "ridge_test_score 1 0.733751425313569\n",
      "ridge_training_score 5 0.9516680923866553\n",
      "ridge_test_score 5 0.7542759407069556\n",
      "ridge_training_score 10 0.9251497005988024\n",
      "ridge_test_score 10 0.7594070695553021\n",
      "ridge_training_score 20 0.8962075848303394\n",
      "ridge_test_score 20 0.7679589509692132\n",
      "svc_training_score 0.8023952095808383\n",
      "svc_test_score 0.7896237172177879\n",
      "svctf_training_score 0.8023952095808383\n",
      "svctf_test_score 0.7896237172177879\n",
      "knn_training_score 1 0.8023952095808383\n",
      "knn_test_score 1 0.7896237172177879\n",
      "knn_training_score 2 0.8023952095808383\n",
      "knn_test_score 2 0.7896237172177879\n",
      "knn_training_score 3 0.8023952095808383\n",
      "knn_test_score 3 0.7896237172177879\n",
      "knn_training_score 4 0.8023952095808383\n",
      "knn_test_score 4 0.7896237172177879\n",
      "knn_training_score 5 0.8023952095808383\n",
      "knn_test_score 5 0.7896237172177879\n",
      "forest_training_score trees:20 dpth: 50 0.8160821214713431\n",
      "forest_test_score trees:20 dpth: 50 0.7890535917901939\n",
      "dtree_training_score  dpth: 50 0.917308240661534\n",
      "dtree_test_score  dpth: 50 0.7491448118586089\n",
      "forest_training_score trees:20 dpth: 100 0.8656971770744226\n",
      "forest_test_score trees:20 dpth: 100 0.7856328392246295\n",
      "dtree_training_score  dpth: 100 0.9620758483033932\n",
      "dtree_test_score  dpth: 100 0.7320410490307868\n",
      "forest_training_score trees:20 dpth: 200 0.9526660963786713\n",
      "forest_test_score trees:20 dpth: 200 0.7833523375142531\n",
      "dtree_training_score  dpth: 200 0.9883090960935272\n",
      "dtree_test_score  dpth: 200 0.7234891676168758\n",
      "forest_training_score trees:20 dpth: 300 0.9807527801539777\n",
      "forest_test_score trees:20 dpth: 300 0.7816419612314709\n",
      "dtree_training_score  dpth: 300 0.9985742800114058\n",
      "dtree_test_score  dpth: 300 0.7189281641961232\n",
      "forest_training_score trees:50 dpth: 50 0.8072426575420587\n",
      "forest_test_score trees:50 dpth: 50 0.7890535917901939\n",
      "forest_training_score trees:50 dpth: 100 0.8632734530938124\n",
      "forest_test_score trees:50 dpth: 100 0.7862029646522235\n",
      "forest_training_score trees:50 dpth: 200 0.9573709723410322\n",
      "forest_test_score trees:50 dpth: 200 0.7839224629418472\n",
      "forest_training_score trees:50 dpth: 300 0.9951525520387796\n",
      "forest_test_score trees:50 dpth: 300 0.7810718358038768\n",
      "forest_training_score trees:70 dpth: 50 0.8059595095523239\n",
      "forest_test_score trees:70 dpth: 50 0.7890535917901939\n",
      "forest_training_score trees:70 dpth: 100 0.8702594810379242\n",
      "forest_test_score trees:70 dpth: 100 0.7873432155074116\n",
      "forest_training_score trees:70 dpth: 200 0.962360992301112\n",
      "forest_test_score trees:70 dpth: 200 0.7810718358038768\n",
      "forest_training_score trees:70 dpth: 300 0.9958654120330767\n",
      "forest_test_score trees:70 dpth: 300 0.7822120866590649\n",
      "forest_training_score trees:100 dpth: 50 0.8052466495580268\n",
      "forest_test_score trees:100 dpth: 50 0.7896237172177879\n",
      "forest_training_score trees:100 dpth: 100 0.8629883090960935\n",
      "forest_test_score trees:100 dpth: 100 0.7879133409350056\n",
      "forest_training_score trees:100 dpth: 200 0.9613629883090961\n",
      "forest_test_score trees:100 dpth: 200 0.7827822120866591\n",
      "forest_training_score trees:100 dpth: 300 0.9955802680353578\n",
      "forest_test_score trees:100 dpth: 300 0.7827822120866591\n"
     ]
    }
   ],
   "source": [
    "for k, v in results.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"datasets\\\\unigram_science.csv\", \"w\")\n",
    "\n",
    "writer = csv.writer(a_file)\n",
    "for k, v in results.items():\n",
    "    writer.writerow([k, v])\n",
    "\n",
    "a_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "could not allocate 41877504 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-3ebe3bc77ebf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbegin_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mforest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[0;32m    388\u001b[0m                              \u001b[1;33m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'threads'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    167\u001b[0m                                                         indices=indices)\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    896\u001b[0m         \"\"\"\n\u001b[0;32m    897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    899\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    387\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.DepthFirstTreeBuilder.build\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.DepthFirstTreeBuilder.build\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.Tree._add_node\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.Tree._resize_c\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_utils.pyx\u001b[0m in \u001b[0;36msklearn.tree._utils.safe_realloc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: could not allocate 41877504 bytes"
     ]
    }
   ],
   "source": [
    "begin_time = dt.datetime.now()\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 100).fit(X_train, y_train)\n",
    "test_model(forest)\n",
    "\n",
    "train_end_time = dt.datetime.now()\n",
    "print('Time spent training (hh:mm:ss):',  train_end_time - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('aaai', 0.43624606175774516)\n",
      "('visualization', 0.3223039299815076)\n",
      "('agedependent', 0.3194857417795549)\n",
      "('ampamp', 0.2989954111354649)\n",
      "('aggravates', 0.29860082349455064)\n",
      "('neuropathic', 0.2947864426034513)\n",
      "('painfree', 0.28919528208244805)\n",
      "('genebased', 0.2883023619813986)\n",
      "('urbanization', 0.2733032167796802)\n",
      "('movement', 0.27223045102284404)\n",
      "\n",
      "\n",
      "('finelycrushed', -0.4696689965724894)\n",
      "('viii', -0.43792346714062)\n",
      "('yeast', -0.3933578605576364)\n",
      "('ancienttimes', -0.38286713799763833)\n",
      "('tetrahydrocannabinol', -0.37844303043497324)\n",
      "('autismrelated', -0.3699753451279366)\n",
      "('heights', -0.35714510334625044)\n",
      "('mitigates', -0.3540053596705342)\n",
      "('halffemale', -0.34956851962079716)\n",
      "('thousand', -0.33847137317057907)\n",
      "('posttime', -0.053531051994896224)\n",
      "('title_length', 0.009213906232209745)\n"
     ]
    }
   ],
   "source": [
    "feature_names_total = vectorizer.get_feature_names()\n",
    "feature_names_total.extend(['posttime', 'title_length'])\n",
    "sorted_coefs_desc = sorted(list(zip(list(feature_names_total), logreg.coef_[0])), key = lambda e: e[1], reverse=True)\n",
    "sorted_coefs_asc = sorted(list(zip(list(feature_names_total), logreg.coef_[0])), key = lambda e: e[1])\n",
    "features_forest = sorted(list(zip(list(feature_names_total), forest.feature_importances_)), key = lambda e: e[1], reverse=True)\n",
    "# print(features_forest[:10], '\\n')\n",
    "\n",
    "for x in range(10):\n",
    "    print(sorted_coefs_desc[x])\n",
    "\n",
    "print('\\n')\n",
    "for x in range(10):\n",
    "    \n",
    "    print(sorted_coefs_asc[x])\n",
    "for x in range(len(sorted_coefs_desc)):\n",
    "    if sorted_coefs_desc[x][0] == 'posttime' :\n",
    "        print(sorted_coefs_desc[x])\n",
    "    else:\n",
    "        pass\n",
    "for x in range(len(sorted_coefs_desc)):\n",
    "    if sorted_coefs_desc[x][0] == 'title_length' :\n",
    "        print(sorted_coefs_desc[x])\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_df['24h_posttime'].value_counts()\n",
    "science_df['body'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
